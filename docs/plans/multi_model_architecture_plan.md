# マルチモデルアーキテクチャ計画書

**作成日**: 2026-01-22
**最終更新**: 2026-01-23
**ステータス**: ✅ Phase 3 完了（Zhipu AI 統合と UI 改善）

---

## 1. 背景と目的

### 1.1 現状の課題

Nexus ArkはGemini APIに強く依存しており、以下の問題が顕在化している：

1. **無料枠の大幅縮小**: Gemini 2.5 Proの無料枠除外、FlashシリーズのRPDが数千→約20/日へ激減
2. **コスト増大**: コンパニオンAIは会話密度が高く、API課金が膨らみやすい
3. **単一障害点**: Gemini API障害時にサービス全停止のリスク
4. **柔軟性欠如**: 内部処理モデルがハードコードされており、ユーザー選択不可

### 1.2 目標

- **脱Gemini依存**: 最終応答モデル＋内部処理モデルの両方でマルチプロバイダ対応
- **コスト最適化**: 質を落とさず低コスト運用を実現
- **ユーザー主権**: モデル選択の自由度を最大化
- **段階的移行**: 既存機能を維持しながら漸進的に強化

---

## 2. 現状アーキテクチャ分析

### 2.1 LLM呼び出しの現状

| 区分 | 現在の実装 | モデル設定場所 |
|------|-----------|---------------|
| **最終応答** | `LLMFactory` (`llm_factory.py`) | ルーム設定（切替可能） |
| **内部処理** | Gemini固定 (`force_google=True`) | `constants.py` ハードコード |
| **Embedding** | API/ローカル切替可能 | `rag_manager.py` |

### 2.2 内部処理モデル（Gemini固定）の用途

| 用途 | 現在のモデル | 定数名 |
|------|-------------|-------|
| **軽量タスク** | `gemini-2.5-flash-lite` | `INTERNAL_PROCESSING_MODEL` |
| **要約・文章生成** | `gemini-2.5-flash` | `SUMMARIZATION_MODEL` |
| **ベクトル埋め込み** | `gemini-embedding-001` | `EMBEDDING_MODEL` |
| **グループ司会** | `gemma-3-12b-it` | `SUPERVISOR_MODEL` |
| **Web検索** | `gemini-2.5-flash` | `SEARCH_MODEL` |

### 2.3 内部処理の呼び出し箇所

- `retrieval_node`: 検索クエリ・Intent分類生成
- `context_generator_node`: 情景描写生成
- `dreaming_manager.py`: 睡眠時記憶整理・要約
- `episodic_memory_manager.py`: エピソード記憶生成
- `rag_manager.py`: ベクトル埋め込み
- `gemini_api.py`: 自動会話要約、読点補正

---

## 3. 提案アーキテクチャ

### 3.1 設計原則

```
┌─────────────────────────────────────────────────────────┐
│                    ユーザー設定層                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐   │
│  │ 最終応答モデル │  │ 内部処理モデル │  │ Embeddingモデル│   │
│  │ (既存機能)    │  │ (新規追加)    │  │ (既存機能)    │   │
│  └──────────────┘  └──────────────┘  └──────────────┘   │
└─────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────┐
│                 InternalLLMManager（新規）                │
│  - プロバイダ切替（Gemini/OpenAI互換/ローカル）            │
│  - フォールバックチェーン                                │
│  - APIキーローリング                                    │
└─────────────────────────────────────────────────────────┘
```

### 3.2 フェーズ分割

#### Phase 1: 基盤整備（APIキー管理強化） [DONE]
- [x] APIキー集約管理UI（`🔑 APIキー / Webhook管理`に統合）
- [x] Gemini APIキーローリング機能
- [x] キー使用状況モニタリング

#### Phase 2: 内部モデル選択機能 [DONE]
- [x] `InternalModelSettings`クラスの導入（`config_manager.py`）
- [x] 内部処理用モデル設定UI（共通設定に追加）
- [x] `LLMFactory`の`force_google`ロジック改修（動的選択対応）
- [x] 設定の永続化とリロード対応の強化

#### Phase 3: 代替プロバイダ統合 [DONE]
- [x] GLM-4対応（Zhipu AI）
  - GLM-4-Flash/FlashX: **内部処理用**（要約、分類、軽量タスク）
  - GLM-4.7: **最終応答候補**（コスパ重視ユーザー向け選択肢）
- [x] APIキー UI の改善と集約
- [ ] Groq対応（高速推論）
- [ ] ローカルLLM対応（Ollama/llama.cpp）

#### Phase 4: フォールバック機構
- [ ] フォールバックチェーン設定UI
- [ ] コスト最適化ダッシュボード（使用量可視化）

> [!NOTE]
> **ユーザーフィードバックに基づく設計変更（2026-01-22）**
> - タスク別ルーティングは不要と判断。最終応答モデルはユーザーが完全に自分で選択する。
> - 内部処理とEmbeddingのみ、コスト最適化のための選択肢を提供。

---

## 4. 詳細設計

### 4.1 価格比較（2026年1月時点）

| モデル | 入力 ($/1M tokens) | 出力 ($/1M tokens) | 備考 |
|--------|-------------------|-------------------|------|
| **Gemini 2.5 Pro** | $1.25 | $10.00 | 200k超: $2.50/$15 |
| **Gemini 3 Flash** | $0.50 | $3.00 | 高速・高コスパ |
| **Gemini 3 Pro** | $2.00 | $12.00 | 200k超: $4/$18 |
| **GLM-4.7** | $0.60 | $2.20 | Gemini 3 Flashより出力安い |
| **GLM-4.7-FlashX** | $0.07 | $0.40 | 内部処理に最適 |
| **GLM-4-Flash** | **無料** | **無料** | 128kコンテキスト |

> [!IMPORTANT]
> **コスト比較分析**
> - **最終応答**: GLM-4.7はGemini 3 Flash（$0.50/$3.00）と比べて入力は割高だが出力は安価
> - **内部処理**: GLM-4-Flash（無料）またはGLM-4.7-FlashX（$0.07/$0.40）が圧倒的にコスト効率良好
> - **結論**: 内部処理にGLM-4-Flash/FlashXを使い、最終応答はユーザー選択に委ねる戦略が最適

> [!TIP]
> **Nexus Ark特有のコスト構造**
>
> Nexus Arkは記憶データ、システムプロンプト、会話履歴などの送信で**入力トークンが出力より圧倒的に多い**。
>
> | シナリオ（入力10k, 出力500 tokens） | Gemini 3 Flash | GLM-4.7 |
> |-----------------------------------------------|----------------|--------|
> | 入力コスト | $0.005 | $0.006 |
> | 出力コスト | $0.0015 | $0.0011 |
> | **合計** | **$0.0065** | **$0.0071** |
>
> → **入力重視のNexus ArkではGemini 3 Flashがコスパ最適**。GLM-4.7は出力量が多いケースで有利。

### 4.2 Phase 1: APIキー管理強化

#### 4.2.1 Gemini APIキーローリング

**目的**: 無料APIキーで一日の上限に達したら次のキーを使用、最後に有料キーを使用

**実装方針**:
```python
# config.json 構造（新規）
{
    "gemini_api_keys": [
        {"key": "AIza...", "type": "free", "priority": 1},
        {"key": "AIza...", "type": "free", "priority": 2},
        {"key": "AIza...", "type": "paid", "priority": 99}
    ],
    "current_key_index": 0,
    "key_exhausted_until": {}  # {"key_hash": "2026-01-22T00:00:00"}
}
```

**対象ファイル**:
- `config_manager.py`: キー管理機能追加
- `llm_factory.py`: キー取得ロジック変更
- `nexus_ark.py`: キー管理UI追加

#### 4.2.2 APIキー設定の集約管理

**現状**: OpenAI互換設定とGemini設定が分散
**目標**: `🔑 APIキー / Webhook管理`アコーディオンに統合

### 4.3 Phase 2: 内部モデル選択機能

#### 4.3.1 設定構造

```python
# constants.py（デフォルト値として維持）
INTERNAL_PROCESSING_MODEL_DEFAULT = "gemini-2.5-flash-lite"
SUMMARIZATION_MODEL_DEFAULT = "gemini-2.5-flash"

# config.json（ユーザー設定）
{
    "internal_model_settings": {
        "provider": "google",  # "google", "zhipu", "openai", "local"
        "processing_model": "gemini-2.5-flash-lite",  # 軽量タスク
        "summarization_model": "gemini-2.5-flash",    # 要約・文章生成
        "supervisor_model": "gemini-2.5-flash-lite",  # グループ司会（新規追加）
        "embedding_model": "gemini-embedding-001",    # Embedding（統合）
        "embedding_mode": "api"  # "api" / "local"
    }
}
```

> [!NOTE]
> **グループ司会モデルの追加**
> - 現在の`gemma-3-12b-it`（Ollama）は処理が遅い
> - `gemini-2.5-flash-lite`程度の軽量モデルで十分（分類タスクのため）
> - ユーザーが選択可能に

> [!NOTE]
> **Embedding設定の統合**
> - 現在「記憶」タブにあるEmbedding設定を、内部モデル設定UIに集約
> - 追加候補: `text-embedding-3-small`（OpenAI）, `multilingual-e5-small/large`

#### 4.3.2 UI設計

```
📋 共通設定
  └── ⚙️ 内部処理モデル設定
        ├── プロバイダ選択: [Google Gemini | Zhipu AI (GLM) | OpenAI互換 | ローカル]
        ├── 軽量処理モデル: [ドロップダウン]
        ├── 要約・文章生成モデル: [ドロップダウン]
        ├── グループ司会モデル: [ドロップダウン]  ← 新規追加
        ├── Embeddingモデル: [ドロップダウン]  ← 集約
        ├── Embeddingモード: [API | ローカル]
        └── [デフォルトに戻す] ボタン
```

### 4.4 Phase 3: 代替プロバイダ統合

#### 4.4.1 推奨用途とモデルの対応

| 用途 | Gemini（現状） | GLM-4（推奨代替） | ローカル（VRAM 4GB） |
|------|---------------|------------------|-------------------|
| **内部処理・軽量タスク** | flash-lite | GLM-4-Flash（無料）/ FlashX | Qwen 2.5 3B GGUF |
| **要約・文章生成** | flash | GLM-4-Flash / FlashX | Qwen 2.5 3B |
| **最終応答（ユーザー選択）** | 各種 | GLM-4.7（選択肢として提供） | - |
| **Embedding** | embedding-001 | - | multilingual-e5-small |

#### 4.4.2 Zhipu AI (GLM-4) 統合

**GLM-4-Flash（内部処理推奨）**:
- 完全無料、128kコンテキスト、72トークン/秒
- 日本語処理品質が高い（漢字処理に強み）
- 要約、Intent分類、クエリ生成など内部タスクに最適

**GLM-4.7（最終応答の選択肢として）**:
- Gemini 3 Flash対比で出力コスト安い（$3.00→$2.20）
- Preserved Thinking機能による深い推論
- 最終応答モデルとしてモデルリストに追加

**実装**:
- `langchain-zhipuai`または`ChatOpenAI`互換エンドポイント使用
- `llm_factory.py`に`zhipu`プロバイダ追加

#### 4.4.3 ローカルLLM対応

**推奨構成（VRAM 4GB環境）**:
- モデル: Qwen 2.5 3B Instruct (GGUF, Q4_K_M)
- **ランタイム: llama-cpp-python（推奨）**
- 用途: 内部処理の補助（クラウドAPIフォールバック先）

**実装方針**:
- **llama-cpp-pythonを優先**（Ollamaより配布・インストールが容易）
  - pip installで完結、別プロセス不要
  - Nexus Arkから直接利用可能
  - CPU版をデフォルト、GPU対応はオプション
- Ollamaは後方互換として維持（既存ユーザー向け）

> [!TIP]
> **llama-cpp-python vs Ollama**
>
> | 観点 | llama-cpp-python | Ollama |
> |------|-----------------|--------|
> | インストール | `pip install` | 別インストーラ |
> | 依存関係 | Pythonライブラリとして統合 | 独立プロセス |
> | 配布 | pipで自動解決 | 各OSインストーラ必要 |
> | ユーザー体験 | Nexus Arkから直接利用 | 別ソフト起動必要 |
>
> → **配布の容易さでllama-cpp-pythonを推奨**

### 4.5 Phase 4: フォールバック機構

#### 4.5.1 フォールバックチェーン

```
1. プライマリ（ユーザー設定）
   ↓ エラー/レート制限
2. セカンダリ（設定可能）
   ↓ エラー/レート制限
3. ターシャリ（Gemini Flash）
   ↓ エラー/レート制限
4. 最終手段（ローカル/エラー通知）
```

---

## 5. 既存タスクとの統合

以下のTASK_LISTの項目を本計画に統合：

| タスク | 優先度 | 本計画での位置づけ |
|--------|-------|------------------|
| 内部仕様モデルのカスタム設定 | 🔴高 | Phase 2 |
| GeminiAPIキーのローリング機能 | 🔴高 | Phase 1 |
| APIキー設定の集約管理 | 🟠高 | Phase 1 |
| Ollamaをllama.cppに置き換える検討 | 🟡中 | Phase 3 検討事項 |
| Google検索を無料GeminiAPIでも使う方法 | 🟠高 | Phase 3 (Grounding機能) |

---

## 6. リスクと緩和策

| リスク | 影響度 | 緩和策 |
|--------|-------|--------|
| 代替モデルの品質低下 | 高 | A/Bテスト、ユーザーフィードバック収集 |
| 設定複雑化によるUX低下 | 中 | 「おすすめ設定」プリセット、デフォルト値維持 |
| ローカルLLMの環境依存 | 中 | Ollama/llama.cpp両対応、明確なドキュメント |
| APIキー漏洩リスク | 高 | ローカル暗号化、UI警告表示 |

---

## 7. 実装優先順位とスケジュール案

### 推奨実装順序

| 順序 | フェーズ | タスク | 理由 |
|------|---------|-------|------|
| 1 | Phase 1 | APIキーローリング | 即効性が高い、既存構造への影響小 |
| 2 | Phase 2 | 内部モデル選択UI | コア機能、これ以降の基盤 |
| 3 | Phase 3a | GLM-4-Flash統合 | 無料・高品質・実装容易 |
| 4 | Phase 3b | ローカルEmbedding強化 | 既存機能の拡張 |
| 5 | Phase 3c | ローカルLLM本格対応 | 環境依存あり、検証必要 |
| 6 | Phase 4 | ハイブリッドルーティング | 全体統合、最終形 |

---

## 8. 検証計画

### 8.1 自動テスト
- [ ] `test_llm_factory.py`: プロバイダ切替、フォールバックのユニットテスト
- [ ] `test_api_key_rotation.py`: キーローリング機能のテスト

### 8.2 手動検証
- [ ] 各プロバイダで基本会話が動作することを確認
- [ ] APIキー上限到達時のフォールバック動作確認
- [ ] ローカルモデルでの応答品質・速度確認

---

## 9. 参考資料

- [Deep Research調査報告書](../research/コンパニオンAIチャットエージェント開発における持続可能なモデル選定とアーキテクチャ最適化に関する詳細調査報告書)
- [TASK_LIST.md](TASK_LIST.md) - 関連タスク項目
- [distribution_system_plan.md](distribution_system_plan.md) - 配布計画

---

> [!NOTE]
> この計画書は「永続的計画」として維持し、各フェーズ完了時に更新します。
> 具体的な実装時には、各フェーズ用の詳細実装計画を別途作成します。
