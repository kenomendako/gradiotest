# rag_manager.py (v6: Incremental Save / Checkpoint System)

import gc
import os
import shutil
import tempfile
import time
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple
import traceback
import logging
import json
import hashlib

from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.document import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from google.api_core import exceptions as google_exceptions

import constants
import config_manager
import utils

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class LangChainEmbeddingWrapper(Embeddings):
    """ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’LangChainã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ãƒ©ãƒƒãƒ—"""
    
    def __init__(self, local_provider):
        self.local_provider = local_provider
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # local_provider.embed_documents ã¯ np.ndarray ã‚’è¿”ã™æƒ³å®š
        embeddings = self.local_provider.embed_documents(texts)
        return embeddings.tolist()
    
    def embed_query(self, text: str) -> List[float]:
        # local_provider.embed_query ã¯ np.ndarray ã‚’è¿”ã™æƒ³å®š
        embedding = self.local_provider.embed_query(text)
        return embedding.tolist()


class RAGManager:
    def __init__(self, room_name: str, api_key: str):
        self.room_name = room_name
        self.api_key = api_key
        self.room_dir = Path(constants.ROOMS_DIR) / room_name
        self.rag_data_dir = self.room_dir / "rag_data"
        
        self.static_index_path = self.rag_data_dir / "faiss_index_static"
        self.dynamic_index_path = self.rag_data_dir / "faiss_index_dynamic"
        self.processed_files_record = self.rag_data_dir / "processed_static_files.json"
        
        self.rag_data_dir.mkdir(parents=True, exist_ok=True)

        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®šã‹ã‚‰å–å¾—
        effective_settings = config_manager.get_effective_settings(room_name)
        self.embedding_mode = effective_settings.get("embedding_mode", "api")
        
        if self.embedding_mode == "local":
            # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
            try:
                from topic_cluster_manager import LocalEmbeddingProvider
                local_provider = LocalEmbeddingProvider()
                self.embeddings = LangChainEmbeddingWrapper(local_provider)
                print(f"[RAGManager] ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–")
            except Exception as e:
                print(f"[RAGManager] ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°åˆæœŸåŒ–å¤±æ•—ã€APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                from langchain_google_genai import GoogleGenerativeAIEmbeddings
                self.embeddings = GoogleGenerativeAIEmbeddings(
                    model=constants.EMBEDDING_MODEL,
                    google_api_key=self.api_key,
                    task_type="retrieval_document"
                )
        else:
            # Gemini API ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
            from langchain_google_genai import GoogleGenerativeAIEmbeddings
            self.embeddings = GoogleGenerativeAIEmbeddings(
                model=constants.EMBEDDING_MODEL,
                google_api_key=self.api_key,
                task_type="retrieval_document"
            )

    def _load_processed_record(self) -> Set[str]:
        if self.processed_files_record.exists():
            try:
                with open(self.processed_files_record, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except Exception:
                return set()
        return set()

    def _save_processed_record(self, processed_files: Set[str]):
        with open(self.processed_files_record, 'w', encoding='utf-8') as f:
            json.dump(list(processed_files), f, indent=2, ensure_ascii=False)

    def _filter_meaningful_chunks(self, splits: List[Document]) -> List[Document]:
        """
        ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å¾Œã«ç„¡æ„å‘³ãªãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–ã™ã‚‹ã€‚
        - çŸ­ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ï¼ˆ10æ–‡å­—æœªæº€ï¼‰
        - ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã®ã¿ã®ãƒãƒ£ãƒ³ã‚¯ï¼ˆ*, -, #, **ç­‰ï¼‰
        """
        MIN_CONTENT_LENGTH = 10
        # é™¤å¤–å¯¾è±¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã®ã¿
        MEANINGLESS_PATTERNS = {'*', '-', '#', '**', '***', '---', '##', '###', '####'}
        
        filtered = []
        filtered_count = 0
        for doc in splits:
            content = doc.page_content.strip()
            # çŸ­ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
            if len(content) < MIN_CONTENT_LENGTH:
                filtered_count += 1
                continue
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ã®ã¿ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
            if content in MEANINGLESS_PATTERNS:
                filtered_count += 1
                continue
            filtered.append(doc)
        
        if filtered_count > 0:
            print(f"    [FILTER] ç„¡æ„å‘³ãªãƒãƒ£ãƒ³ã‚¯ {filtered_count}ä»¶ã‚’é™¤å¤–")
        
        return filtered

    def classify_query_intent(self, query: str) -> dict:
        """
        ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†é¡ã—ã€Intent-Aware Retrievalã®é‡ã¿ã‚’è¿”ã™ã€‚
        
        Returns:
            {
                "intent": "emotional" | "factual" | "technical" | "temporal" | "relational",
                "weights": {"alpha": float, "beta": float, "gamma": float}
            }
        """
        try:
            from gemini_api import get_configured_llm
            
            llm = get_configured_llm(constants.INTERNAL_PROCESSING_MODEL, self.api_key, {})
            
            prompt = """ã‚ãªãŸã¯ã‚¯ã‚¨ãƒªåˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã‚’5ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ã‚«ãƒ†ã‚´ãƒª:
- emotional: æ„Ÿæƒ…ãƒ»ä½“é¨“ãƒ»æ€ã„å‡ºã‚’å•ã†ï¼ˆä¾‹ï¼šã€Œã‚ã®æ™‚ã©ã†æ€ã£ãŸï¼Ÿã€ã€Œå¬‰ã—ã‹ã£ãŸã“ã¨ã€ã€Œåˆã‚ã¦ä¼šã£ãŸæ—¥ã€ï¼‰
- factual: äº‹å®Ÿãƒ»å±æ€§ã‚’å•ã†ï¼ˆä¾‹ï¼šã€ŒçŒ«ã®åå‰ã¯ï¼Ÿã€ã€Œèª•ç”Ÿæ—¥ã„ã¤ï¼Ÿã€ã€Œå¥½ããªé£Ÿã¹ç‰©ã€ï¼‰
- technical: æŠ€è¡“ãƒ»æ‰‹é †ãƒ»è¨­å®šã‚’å•ã†ï¼ˆä¾‹ï¼šã€Œè¨­å®šæ–¹æ³•ã¯ï¼Ÿã€ã€Œã©ã†ã‚„ã£ã¦å‹•ã‹ã™ï¼Ÿã€ã€Œãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€ï¼‰
- temporal: æ™‚é–“è»¸ã§å•ã†ï¼ˆä¾‹ï¼šã€Œæœ€è¿‘ä½•ã—ãŸï¼Ÿã€ã€Œæ˜¨æ—¥ã®è©±ã€ã€Œä»Šé€±ã®äºˆå®šã€ï¼‰
- relational: é–¢ä¿‚æ€§ã‚’å•ã†ï¼ˆä¾‹ï¼šã€Œã€‡ã€‡ã¨ã®é–¢ä¿‚ã¯ï¼Ÿã€ã€Œèª°ã¨ä»²è‰¯ã„ï¼Ÿã€ã€Œã©ã‚“ãªäººï¼Ÿã€ï¼‰

ã‚¯ã‚¨ãƒª: {query}

ã‚«ãƒ†ã‚´ãƒªåã®ã¿ã‚’1å˜èªã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆemotional/factual/technical/temporal/relationalï¼‰:"""

            response = llm.invoke(prompt.format(query=query)).content.strip().lower()
            
            # å¿œç­”ã‹ã‚‰Intentã‚’æŠ½å‡º
            intent = constants.DEFAULT_INTENT
            for valid_intent in constants.INTENT_WEIGHTS.keys():
                if valid_intent in response:
                    intent = valid_intent
                    break
            
            weights = constants.INTENT_WEIGHTS.get(intent, constants.INTENT_WEIGHTS[constants.DEFAULT_INTENT])
            print(f"  - [Intent] Query: '{query[:30]}...' -> {intent} (Î±={weights['alpha']}, Î²={weights['beta']}, Î³={weights['gamma']})")
            
            return {"intent": intent, "weights": weights}
            
        except Exception as e:
            print(f"  - [Intent] åˆ†é¡ã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨: {e}")
            return {
                "intent": constants.DEFAULT_INTENT,
                "weights": constants.INTENT_WEIGHTS[constants.DEFAULT_INTENT]
            }

    def calculate_time_decay(self, metadata: dict) -> float:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã‹ã‚‰æ™‚é–“æ¸›è¡°ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        
        Args:
            metadata: {"date": "2026-01-15", ...} ã¾ãŸã¯ {"created_at": "2026-01-15 10:00:00", ...}
        
        Returns:
            0.0ï¼ˆéå¸¸ã«å¤ã„ï¼‰ï½ 1.0ï¼ˆä»Šæ—¥ï¼‰
        """
        import math
        from datetime import datetime, timedelta
        
        # æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œï¼‰
        date_str = metadata.get("date") or metadata.get("created_at", "")
        
        if not date_str:
            return 0.5  # æ—¥ä»˜ä¸æ˜ã¯ä¸­ç«‹
        
        try:
            # æ—¥ä»˜éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºï¼ˆ"2026-01-15" or "2026-01-15 10:00:00"ï¼‰
            date_part = str(date_str).split()[0]
            
            # æ—¥ä»˜ç¯„å›²ã®å ´åˆï¼ˆ"2026-01-01~2026-01-07"ï¼‰ã¯æœ€æ–°æ—¥ã‚’ä½¿ç”¨
            if "~" in date_part:
                date_part = date_part.split("~")[-1]
            
            record_date = datetime.strptime(date_part, "%Y-%m-%d")
            today = datetime.now()
            days_ago = (today - record_date).days
            
            if days_ago < 0:
                return 1.0  # æœªæ¥ã®æ—¥ä»˜ã¯æœ€æ–°æ‰±ã„
            
            # æŒ‡æ•°æ¸›è¡°: decay = e^(-rate Ã— days)
            decay_score = math.exp(-constants.TIME_DECAY_RATE * days_ago)
            return decay_score
            
        except Exception as e:
            # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¸­ç«‹
            return 0.5

    def _safe_save_index(self, db: FAISS, target_path: Path):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å®‰å…¨ã«ä¿å­˜ã™ã‚‹ï¼ˆãƒªãƒãƒ¼ãƒ é€€é¿æ–¹å¼ï¼‰"""
        target_path = Path(target_path)
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            db.save_local(str(temp_path))
            
            # Windowsã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯å•é¡Œã«å¯¾å¿œ
            max_retries = 3
            for attempt in range(max_retries):
                # é€€é¿ç”¨ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
                old_path = target_path.parent / (target_path.name + f".old_{int(time.time())}_{attempt}")
                
                try:
                    if target_path.exists():
                        # 1. ã¾ãšãƒªãƒãƒ¼ãƒ ã‚’è©¦ã¿ã‚‹ï¼ˆãƒ•ã‚©ãƒ«ãƒ€ãŒé–‹ã‹ã‚Œã¦ã„ã¦ã‚‚ãƒªãƒãƒ¼ãƒ ã¯æˆåŠŸã™ã‚‹ã“ã¨ãŒå¤šã„ï¼‰
                        try:
                            target_path.rename(old_path)
                            # print(f"  - [RAG] æ—¢å­˜ç´¢å¼•ã‚’ãƒªãƒãƒ¼ãƒ é€€é¿: {old_path.name}")
                        except Exception:
                            # 2. ãƒªãƒãƒ¼ãƒ ã«å¤±æ•—ã—ãŸå ´åˆã¯ GC ã‚’å‘¼ã‚“ã§ã‹ã‚‰å‰Šé™¤ã‚’è©¦è¡Œï¼ˆå¾“æ¥æ–¹å¼ï¼‰
                            gc.collect()
                            time.sleep(0.5)
                            shutil.rmtree(str(target_path))
                    
                    # 3. æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é…ç½®
                    shutil.move(str(temp_path), str(target_path))
                    
                    # 4. é€€é¿ã—ãŸå¤ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‰Šé™¤ã‚’è©¦ã¿ã‚‹ï¼ˆå¤±æ•—ã—ã¦ã‚‚ç´¢å¼•æ›´æ–°è‡ªä½“ã¯æˆåŠŸã¨ã™ã‚‹ï¼‰
                    self._cleanup_old_indices(target_path.parent, target_path.name)
                    return # æˆåŠŸ
                    
                except PermissionError as e:
                    if attempt < max_retries - 1:
                        wait_time = 1 * (attempt + 1)
                        print(f"  - [RAG] ä¿å­˜å¾…æ©Ÿä¸­... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        print(f"  - [RAG] ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒä½¿ç”¨ä¸­ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚: {e}")
                        raise

    def _cleanup_old_indices(self, parent_dir: Path, base_name: str):
        """é€€é¿ã•ã‚ŒãŸå¤ã„ .old ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹"""
        try:
            for old_dir in parent_dir.glob(f"{base_name}.old_*"):
                if old_dir.is_dir():
                    try:
                        shutil.rmtree(str(old_dir))
                    except Exception:
                        pass # å‰Šé™¤ã§ããªã„å ´åˆã¯è«¦ã‚ã‚‹ï¼ˆæ¬¡å›ä»¥é™ã«æœŸå¾…ï¼‰
        except Exception:
            pass

    def _safe_load_index(self, target_path: Path) -> Optional[FAISS]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å®‰å…¨ã«èª­ã¿è¾¼ã‚€ï¼ˆä¸€æ™‚ã‚³ãƒ”ãƒ¼çµŒç”±ï¼‰"""
        if not target_path.exists():
            return None
            
        # ãƒ­ãƒ¼ãƒ‰ä¸­ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¶ˆå¤±ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã€ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦ãƒ­ãƒ¼ãƒ‰
        # æ³¨æ„: FAISS.load_local ã¯ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¿…è¦ã¨ã—ãªã„ãŸã‚ã€
        # ã“ã“ã§ã¯ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã™ã‚Œã°ååˆ†ã€‚
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_index_path = Path(temp_dir) / "index"
            try:
                shutil.copytree(str(target_path), str(temp_index_path))
                
                # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
                return FAISS.load_local(
                    str(temp_index_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
            except Exception as e:
                # print(f"  - [RAG] ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return None

    def _create_index_in_batches(self, splits: List[Document], existing_db: Optional[FAISS] = None, 
                                   progress_callback=None, save_callback=None, status_callback=None) -> FAISS:
        """
        å¤§é‡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒƒãƒåˆ†å‰²ã—ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿ã—ãªãŒã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ/è¿½è¨˜ã™ã‚‹ã€‚
        progress_callback: é€²æ—ã‚’å ±å‘Šã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (batch_num, total_batches) -> None
        save_callback: é€”ä¸­ä¿å­˜ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (db) -> Noneï¼ˆå®šæœŸçš„ã«å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼‰
        status_callback: UIã¸é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (message) -> None
        """
        BATCH_SIZE = 20
        SAVE_INTERVAL_BATCHES = 20  # 20ãƒãƒƒãƒã”ã¨ï¼ˆç´„40ç§’é–“éš”ï¼‰ã«é€”ä¸­ä¿å­˜ãƒ»é€²æ—å ±å‘Š
        db = existing_db
        total_splits = len(splits)
        total_batches = (total_splits + BATCH_SIZE - 1) // BATCH_SIZE
        
        print(f"    [BATCH] é–‹å§‹: {total_splits} ãƒãƒ£ãƒ³ã‚¯, {total_batches} ãƒãƒƒãƒ (é€”ä¸­ä¿å­˜: {SAVE_INTERVAL_BATCHES}ãƒãƒƒãƒã”ã¨)")
        if status_callback:
            status_callback(f"ç´¢å¼•å‡¦ç†é–‹å§‹: {total_splits}ãƒãƒ£ãƒ³ã‚¯, {total_batches}ãƒãƒƒãƒ")
        if progress_callback:
            progress_callback(0, total_batches)

        for i in range(0, total_splits, BATCH_SIZE):
            batch = splits[i : i + BATCH_SIZE]
            batch_num = (i // BATCH_SIZE) + 1
            
            # ãƒªãƒˆãƒ©ã‚¤ãƒ«ãƒ¼ãƒ—
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    if db is None:
                        db = FAISS.from_documents(batch, self.embeddings)
                    else:
                        db.add_documents(batch)
                    
                    # é€²æ—ã‚’å ±å‘Š
                    if progress_callback:
                        progress_callback(batch_num, total_batches)
                    
                    if self.embedding_mode == "api":
                        time.sleep(2) 
                    break 
                
                except Exception as e:
                    error_str = str(e)
                    print(f"      ! ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{max_retries}): {e}")
                    if "429" in error_str or "ResourceExhausted" in error_str:
                        wait_time = 10 * (attempt + 1)
                        print(f"      ! APIåˆ¶é™æ¤œçŸ¥ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                        if status_callback:
                            status_callback(f"APIåˆ¶é™ - {wait_time}ç§’å¾…æ©Ÿä¸­...")
                        time.sleep(wait_time)
                    else:
                        if attempt == max_retries - 1:
                            print(f"      ! ã“ã®ãƒãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚æœ€çµ‚ã‚¨ãƒ©ãƒ¼: {e}")
                            traceback.print_exc()
                        if self.embedding_mode == "api":
                            time.sleep(2)
            
            # å®šæœŸé€²æ—å ±å‘Šã¨é€”ä¸­ä¿å­˜ï¼ˆ100ãƒãƒƒãƒã”ã¨ï¼‰
            if batch_num % SAVE_INTERVAL_BATCHES == 0:
                progress_pct = int((batch_num / total_batches) * 100)
                print(f"    [PROGRESS] {batch_num}/{total_batches} ãƒãƒƒãƒå®Œäº† ({progress_pct}%)")
                if status_callback:
                    status_callback(f"ç´¢å¼•å‡¦ç†ä¸­: {batch_num}/{total_batches} ({progress_pct}%)")
                # é€”ä¸­ä¿å­˜
                if save_callback and db:
                    print(f"    [SAVE] é€”ä¸­ä¿å­˜å®Ÿè¡Œ...")
                    save_callback(db)
        
        print(f"    [BATCH] å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº†")
        return db


    def update_memory_index(self, status_callback=None) -> str:
        """
        è¨˜æ†¶ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã™ã‚‹ï¼ˆéå»ãƒ­ã‚°ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã€å¤¢æ—¥è¨˜ã€æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        """
        def report(message):
            print(f"--- [RAG Memory] {message}")
            if status_callback: status_callback(message)

        report("è¨˜æ†¶ç´¢å¼•ã‚’æ›´æ–°ä¸­: éå»ãƒ­ã‚°ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã€å¤¢æ—¥è¨˜ã€æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å·®åˆ†ã‚’ç¢ºèª...")
        
        processed_records = self._load_processed_record()
        
        # å‡¦ç†å¯¾è±¡ã®ã‚­ãƒ¥ãƒ¼: (record_id, document) ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ
        pending_items: List[Tuple[str, Document]] = []
        
        # 1. éå»ãƒ­ã‚°åé›†
        archives_dir = self.room_dir / "log_archives"
        if archives_dir.exists():
            for f in list(archives_dir.glob("*.txt")):
                record_id = f"archive:{f.name}"
                if record_id not in processed_records:
                    try:
                        content = f.read_text(encoding="utf-8")
                        if content.strip():
                            doc = Document(page_content=content, metadata={"source": f.name, "type": "log_archive", "path": str(f)})
                            pending_items.append((record_id, doc))
                    except Exception: pass

        # 2. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶åé›†
        episodic_memory_path = self.room_dir / "memory" / "episodic_memory.json"
        if episodic_memory_path.exists():
            try:
                with open(episodic_memory_path, 'r', encoding='utf-8') as f:
                    episodes = json.load(f)
                if isinstance(episodes, list):
                    for ep in episodes:
                        date_str = ep.get('date', 'unknown')
                        record_id = f"episodic:{date_str}"
                        if record_id not in processed_records:
                            summary = ep.get('summary', '')
                            if summary:
                                content = f"æ—¥ä»˜: {date_str}\nå†…å®¹: {summary}"
                                doc = Document(page_content=content, metadata={"source": "episodic_memory.json", "type": "episodic_memory", "date": date_str})
                                pending_items.append((record_id, doc))
            except Exception: pass

        # 3. å¤¢æ—¥è¨˜åé›†
        insights_path = self.room_dir / "memory" / "insights.json"
        if insights_path.exists():
            try:
                with open(insights_path, 'r', encoding='utf-8') as f:
                    insights = json.load(f)
                if isinstance(insights, list):
                    for item in insights:
                        date_str = item.get('created_at', '').split(' ')[0]
                        record_id = f"dream:{date_str}"
                        if record_id not in processed_records:
                            insight_content = item.get('insight', '')
                            strategy = item.get('strategy', '')
                            if insight_content:
                                content = f"ã€éå»ã®å¤¢ãƒ»æ·±å±¤å¿ƒç†ã®è¨˜éŒ² ({date_str})ã€‘\nãƒˆãƒªã‚¬ãƒ¼: {item.get('trigger_topic','')}\næ°—ã¥ã: {insight_content}\næŒ‡é‡: {strategy}"
                                doc = Document(page_content=content, metadata={"source": "insights.json", "type": "dream_insight", "date": date_str})
                                pending_items.append((record_id, doc))
            except Exception: pass

        # 4. æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆmemory_main.txt + memory_archived_*.txtï¼‰
        diary_dir = self.room_dir / "memory"
        if diary_dir.exists():
            for f in diary_dir.glob("memory*.txt"):
                # memory_main.txt ã¨ memory_archived_*.txt ãŒå¯¾è±¡
                if f.name.startswith("memory") and f.name.endswith(".txt"):
                    try:
                        content = f.read_text(encoding="utf-8")
                        if content.strip():
                            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒãƒƒã‚·ãƒ¥ã§record_idã‚’ç”Ÿæˆï¼ˆå¤‰æ›´æ¤œå‡ºç”¨ï¼‰
                            content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
                            record_id = f"diary:{f.name}:{content_hash}"
                            if record_id not in processed_records:
                                doc = Document(
                                    page_content=content,
                                    metadata={
                                        "source": f.name,
                                        "type": "diary",  # æ—¥è¨˜ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                                        "path": str(f)
                                    }
                                )
                                pending_items.append((record_id, doc))
                    except Exception as e:
                        print(f"  - æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({f.name}): {e}")

        # [2026-01-10] ç ”ç©¶ãƒ»åˆ†æãƒãƒ¼ãƒˆåé›†
        research_notes_path = self.room_dir / constants.RESEARCH_NOTES_FILENAME
        if research_notes_path.exists():
            try:
                content = research_notes_path.read_text(encoding="utf-8")
                if content.strip():
                    content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
                    record_id = f"research_notes:{content_hash}"
                    if record_id not in processed_records:
                        doc = Document(
                            page_content=content,
                            metadata={
                                "source": constants.RESEARCH_NOTES_FILENAME,
                                "type": "research_notes",
                                "path": str(research_notes_path)
                            }
                        )
                        pending_items.append((record_id, doc))
            except Exception as e:
                print(f"  - ç ”ç©¶ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # 5. ç¾è¡Œãƒ­ã‚° (log.txt) - å‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§å‡¦ç†ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯é™¤å¤–
        # ç¾è¡Œãƒ­ã‚°ã¯é »ç¹ã«å¤‰æ›´ã•ã‚Œã‚‹ãŸã‚ã€æ¯å›å†æ§‹ç¯‰ã™ã‚‹å‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å´ã§å‡¦ç†ã™ã‚‹æ–¹ãŒåŠ¹ç‡çš„

        # --- å®Ÿè¡Œ: å°åˆ†ã‘ã«ã—ã¦ä¿å­˜ã—ãªãŒã‚‰é€²ã‚€ ---
        if pending_items:
            total_pending = len(pending_items)
            report(f"æ–°è¦è¿½åŠ ã‚¢ã‚¤ãƒ†ãƒ : {total_pending}ä»¶ã€‚å‡¦ç†ä¸­...")
            
            static_db = self._safe_load_index(self.static_index_path)
            SAVE_INTERVAL = 5 
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
            processed_count = 0
            
            # é€”ä¸­ä¿å­˜ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            def interim_save(db):
                self._safe_save_index(db, self.static_index_path)
            
            for i in range(0, total_pending, SAVE_INTERVAL):
                batch_items = pending_items[i : i + SAVE_INTERVAL]
                batch_docs = [item[1] for item in batch_items]
                batch_ids = [item[0] for item in batch_items]
                
                print(f"  - ã‚°ãƒ«ãƒ¼ãƒ—å‡¦ç†ä¸­ ({i+1}ã€œ{min(i+SAVE_INTERVAL, total_pending)} / {total_pending})...")
                splits = text_splitter.split_documents(batch_docs)
                splits = self._filter_meaningful_chunks(splits)  # [2026-01-09] ç„¡æ„å‘³ãªãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
                static_db = self._create_index_in_batches(
                    splits, 
                    existing_db=static_db,
                    save_callback=interim_save,
                    status_callback=status_callback
                )
                
                if static_db:
                    self._safe_save_index(static_db, self.static_index_path)
                    processed_records.update(batch_ids)
                    self._save_processed_record(processed_records)
                    processed_count += len(batch_items)
                else:
                    print(f"    ! ã‚°ãƒ«ãƒ¼ãƒ—å‡¦ç†å¤±æ•—ã€‚")

            result_msg = f"è¨˜æ†¶ç´¢å¼•: {processed_count}ä»¶ã‚’è¿½åŠ ä¿å­˜"
        else:
            result_msg = "è¨˜æ†¶ç´¢å¼•: å·®åˆ†ãªã—"
        
        print(f"--- [RAG Memory] å®Œäº†: {result_msg} ---")
        return result_msg

    def update_memory_index_with_progress(self):
        """
        è¨˜æ†¶ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã™ã‚‹ï¼ˆé€²æ—ã‚’yieldã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ç‰ˆï¼‰
        yields: (current_step, total_steps, status_message)
        """
        yield (0, 0, "è¨˜æ†¶ç´¢å¼•ã‚’æ›´æ–°ä¸­: å·®åˆ†ã‚’ç¢ºèª...")
        
        processed_records = self._load_processed_record()
        pending_items: List[Tuple[str, Document]] = []
        
        # 1. éå»ãƒ­ã‚°åé›†
        archives_dir = self.room_dir / "log_archives"
        if archives_dir.exists():
            for f in list(archives_dir.glob("*.txt")):
                record_id = f"archive:{f.name}"
                if record_id not in processed_records:
                    try:
                        content = f.read_text(encoding="utf-8")
                        if content.strip():
                            doc = Document(page_content=content, metadata={"source": f.name, "type": "log_archive", "path": str(f)})
                            pending_items.append((record_id, doc))
                    except Exception: pass

        # 2. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶åé›†
        episodic_memory_path = self.room_dir / "memory" / "episodic_memory.json"
        if episodic_memory_path.exists():
            try:
                with open(episodic_memory_path, 'r', encoding='utf-8') as f:
                    episodes = json.load(f)
                if isinstance(episodes, list):
                    for ep in episodes:
                        date_str = ep.get('date', 'unknown')
                        record_id = f"episodic:{date_str}"
                        if record_id not in processed_records:
                            summary = ep.get('summary', '')
                            if summary:
                                content = f"æ—¥ä»˜: {date_str}\nå†…å®¹: {summary}"
                                doc = Document(page_content=content, metadata={"source": "episodic_memory.json", "type": "episodic_memory", "date": date_str})
                                pending_items.append((record_id, doc))
            except Exception: pass

        # 3. å¤¢æ—¥è¨˜åé›†
        insights_path = self.room_dir / "memory" / "insights.json"
        if insights_path.exists():
            try:
                with open(insights_path, 'r', encoding='utf-8') as f:
                    insights = json.load(f)
                if isinstance(insights, list):
                    for item in insights:
                        date_str = item.get('created_at', '').split(' ')[0]
                        record_id = f"dream:{date_str}"
                        if record_id not in processed_records:
                            insight_content = item.get('insight', '')
                            strategy = item.get('strategy', '')
                            if insight_content:
                                content = f"ã€éå»ã®å¤¢ãƒ»æ·±å±¤å¿ƒç†ã®è¨˜éŒ² ({date_str})ã€‘\nãƒˆãƒªã‚¬ãƒ¼: {item.get('trigger_topic','')}\næ°—ã¥ã: {insight_content}\næŒ‡é‡: {strategy}"
                                doc = Document(page_content=content, metadata={"source": "insights.json", "type": "dream_insight", "date": date_str})
                                pending_items.append((record_id, doc))
            except Exception: pass

        # 4. æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«åé›†
        diary_dir = self.room_dir / "memory"
        if diary_dir.exists():
            for f in diary_dir.glob("memory*.txt"):
                if f.name.startswith("memory") and f.name.endswith(".txt"):
                    try:
                        content = f.read_text(encoding="utf-8")
                        if content.strip():
                            content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
                            record_id = f"diary:{f.name}:{content_hash}"
                            if record_id not in processed_records:
                                doc = Document(
                                    page_content=content,
                                    metadata={"source": f.name, "type": "diary", "path": str(f)}
                                )
                                pending_items.append((record_id, doc))
                    except Exception as e:
                        print(f"  - æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({f.name}): {e}")

        # [2026-01-10] ç ”ç©¶ãƒ»åˆ†æãƒãƒ¼ãƒˆåé›†
        research_notes_path = self.room_dir / constants.RESEARCH_NOTES_FILENAME
        if research_notes_path.exists():
            try:
                content = research_notes_path.read_text(encoding="utf-8")
                if content.strip():
                    content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
                    record_id = f"research_notes:{content_hash}"
                    if record_id not in processed_records:
                        doc = Document(
                            page_content=content,
                            metadata={
                                "source": constants.RESEARCH_NOTES_FILENAME,
                                "type": "research_notes",
                                "path": str(research_notes_path)
                            }
                        )
                        pending_items.append((record_id, doc))
            except Exception as e:
                print(f"  - ç ”ç©¶ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        if not pending_items:
            yield (0, 0, "è¨˜æ†¶ç´¢å¼•: å·®åˆ†ãªã—")
            return

        total_pending = len(pending_items)
        yield (0, total_pending, f"æ–°è¦è¿½åŠ ã‚¢ã‚¤ãƒ†ãƒ : {total_pending}ä»¶ã€‚å‡¦ç†ä¸­...")
        
        static_db = self._safe_load_index(self.static_index_path)
        SAVE_INTERVAL = 5
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        processed_count = 0
        
        for i in range(0, total_pending, SAVE_INTERVAL):
            batch_items = pending_items[i : i + SAVE_INTERVAL]
            batch_docs = [item[1] for item in batch_items]
            batch_ids = [item[0] for item in batch_items]
            
            group_num = (i // SAVE_INTERVAL) + 1
            total_groups = (total_pending + SAVE_INTERVAL - 1) // SAVE_INTERVAL
            
            yield (group_num, total_groups, f"ã‚°ãƒ«ãƒ¼ãƒ— {group_num}/{total_groups} å‡¦ç†ä¸­...")
            
            splits = text_splitter.split_documents(batch_docs)
            splits = self._filter_meaningful_chunks(splits)
            
            # ãƒãƒƒãƒå‡¦ç†ï¼ˆé€”ä¸­ä¿å­˜ä»˜ãï¼‰
            BATCH_SIZE = 20
            total_batches = (len(splits) + BATCH_SIZE - 1) // BATCH_SIZE
            
            for j in range(0, len(splits), BATCH_SIZE):
                batch = splits[j : j + BATCH_SIZE]
                batch_num = (j // BATCH_SIZE) + 1
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        if static_db is None:
                            static_db = FAISS.from_documents(batch, self.embeddings)
                        else:
                            static_db.add_documents(batch)
                        
                        if self.embedding_mode == "api":
                            time.sleep(2)
                        break
                    except Exception as e:
                        error_str = str(e)
                        print(f"      ! ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{max_retries}): {e}")
                        if "429" in error_str or "ResourceExhausted" in error_str:
                            wait_time = 10 * (attempt + 1)
                            yield (group_num, total_groups, f"APIåˆ¶é™ - {wait_time}ç§’å¾…æ©Ÿä¸­...")
                            time.sleep(wait_time)
                        else:
                            if attempt == max_retries - 1:
                                print(f"      ! ã“ã®ãƒãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚æœ€çµ‚ã‚¨ãƒ©ãƒ¼: {e}")
                            if self.embedding_mode == "api":
                                time.sleep(2)
                
                # 20ãƒãƒƒãƒã”ã¨ã«é€”ä¸­ä¿å­˜ã¨é€²æ—å ±å‘Š
                if batch_num % 20 == 0:
                    progress_pct = int((batch_num / total_batches) * 100)
                    yield (group_num, total_groups, f"ã‚°ãƒ«ãƒ¼ãƒ— {group_num}: {batch_num}/{total_batches} ãƒãƒƒãƒ ({progress_pct}%)")
                    if static_db:
                        self._safe_save_index(static_db, self.static_index_path)
            
            # ã‚°ãƒ«ãƒ¼ãƒ—å®Œäº†æ™‚ã«ä¿å­˜
            if static_db:
                self._safe_save_index(static_db, self.static_index_path)
                processed_records.update(batch_ids)
                self._save_processed_record(processed_records)
                processed_count += len(batch_items)
        
        result_msg = f"è¨˜æ†¶ç´¢å¼•: {processed_count}ä»¶ã‚’è¿½åŠ ä¿å­˜"
        print(f"--- [RAG Memory] å®Œäº†: {result_msg} ---")
        yield (total_pending, total_pending, result_msg)

    def update_knowledge_index(self, status_callback=None) -> str:
        """
        çŸ¥è­˜ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã™ã‚‹ï¼ˆknowledgeãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¿ï¼‰
        """
        def report(message):
            print(f"--- [RAG Knowledge] {message}")
            if status_callback: status_callback(message)

        report("çŸ¥è­˜ç´¢å¼•ã‚’å†æ§‹ç¯‰ä¸­...")
        dynamic_docs = []
        
        knowledge_dir = self.room_dir / "knowledge"
        if knowledge_dir.exists():
            for f in list(knowledge_dir.glob("*.txt")) + list(knowledge_dir.glob("*.md")):
                try:
                    content = f.read_text(encoding="utf-8")
                    dynamic_docs.append(Document(page_content=content, metadata={"source": f.name, "type": "knowledge"}))
                except Exception: pass

        # çŸ¥è­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¿å‡¦ç†ï¼ˆç¾è¡Œãƒ­ã‚°ã¯åˆ¥ãƒœã‚¿ãƒ³ã§å‡¦ç†ï¼‰
        if dynamic_docs:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
            dynamic_splits = text_splitter.split_documents(dynamic_docs)
            dynamic_splits = self._filter_meaningful_chunks(dynamic_splits)  # [2026-01-09] ç„¡æ„å‘³ãªãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
            
            # é€”ä¸­ä¿å­˜ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            def interim_save(db):
                self._safe_save_index(db, self.dynamic_index_path)
            
            dynamic_db = self._create_index_in_batches(
                dynamic_splits, 
                existing_db=None,
                save_callback=interim_save,
                status_callback=status_callback
            )
            
            if dynamic_db:
                self._safe_save_index(dynamic_db, self.dynamic_index_path)
                result_msg = f"çŸ¥è­˜ç´¢å¼•: {len(dynamic_docs)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"
            else:
                result_msg = "çŸ¥è­˜ç´¢å¼•: ä½œæˆå¤±æ•—"
        else:
            if self.dynamic_index_path.exists():
                shutil.rmtree(str(self.dynamic_index_path))
            result_msg = "çŸ¥è­˜ç´¢å¼•: å¯¾è±¡ãªã—"

        print(f"--- [RAG Knowledge] å®Œäº†: {result_msg} ---")
        return result_msg

    def update_current_log_index_with_progress(self):
        """
        ç¾è¡Œãƒ­ã‚°ï¼ˆlog.txtï¼‰ã®ã¿ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ï¼ˆé€²æ—ã‚’yieldã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ç‰ˆï¼‰
        yields: (batch_num, total_batches, status_message)
        """
        current_log_path = self.room_dir / "log.txt"
        if not current_log_path.exists():
            yield (0, 0, "ç¾è¡Œãƒ­ã‚°: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return
        
        try:
            content = current_log_path.read_text(encoding="utf-8")
            
            if not content.strip():
                yield (0, 0, "ç¾è¡Œãƒ­ã‚°: ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")
                return
            
            doc = Document(page_content=content, metadata={"source": "log.txt", "type": "current_log"})
            
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
            splits = text_splitter.split_documents([doc])
            splits = self._filter_meaningful_chunks(splits)  # [2026-01-09] ç„¡æ„å‘³ãªãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
            
            BATCH_SIZE = 20
            total_batches = (len(splits) + BATCH_SIZE - 1) // BATCH_SIZE
            
            yield (0, total_batches, f"é–‹å§‹: {len(splits)}ãƒãƒ£ãƒ³ã‚¯, {total_batches}ãƒãƒƒãƒ")
            
            db = None
            for i in range(0, len(splits), BATCH_SIZE):
                batch = splits[i : i + BATCH_SIZE]
                batch_num = (i // BATCH_SIZE) + 1
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        if db is None:
                            db = FAISS.from_documents(batch, self.embeddings)
                        else:
                            db.add_documents(batch)
                        
                        yield (batch_num, total_batches, f"å‡¦ç†ä¸­: {batch_num}/{total_batches} ãƒãƒƒãƒå®Œäº†")
                        if self.embedding_mode == "api":
                            time.sleep(2)
                        break
                    except Exception as e:
                        error_str = str(e)
                        print(f"      ! [CurrentLog] ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/{max_retries}): {e}")
                        if "429" in error_str or "ResourceExhausted" in error_str:
                            wait_time = 10 * (attempt + 1)
                            yield (batch_num, total_batches, f"APIåˆ¶é™ - {wait_time}ç§’å¾…æ©Ÿä¸­...")
                            time.sleep(wait_time)
                        else:
                            if attempt == max_retries - 1:
                                yield (batch_num, total_batches, f"ã‚¨ãƒ©ãƒ¼: ãƒãƒƒãƒ{batch_num}ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                                print(f"      ! ã“ã®ãƒãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚æœ€çµ‚ã‚¨ãƒ©ãƒ¼: {e}")
                                traceback.print_exc()
                            if self.embedding_mode == "api":
                                time.sleep(2)
            
            if db:
                current_log_index_path = self.room_dir / "rag_data" / "current_log_index"
                self._safe_save_index(db, current_log_index_path)
                yield (total_batches, total_batches, f"âœ… ç¾è¡Œãƒ­ã‚°: {len(splits)}ãƒãƒ£ãƒ³ã‚¯ã‚’ç´¢å¼•åŒ–å®Œäº†")
            else:
                yield (0, total_batches, "ç¾è¡Œãƒ­ã‚°: ç´¢å¼•åŒ–å¤±æ•—")
                
        except Exception as e:
            import traceback
            traceback.print_exc()
            yield (0, 0, f"ã‚¨ãƒ©ãƒ¼: {e}")

    def create_or_update_index(self, status_callback=None) -> str:
        """
        å¾Œæ–¹äº’æ›ç”¨ãƒ©ãƒƒãƒ‘ãƒ¼: è¨˜æ†¶ç´¢å¼•ã¨çŸ¥è­˜ç´¢å¼•ã®ä¸¡æ–¹ã‚’æ›´æ–°ã™ã‚‹
        """
        memory_result = self.update_memory_index(status_callback)
        knowledge_result = self.update_knowledge_index(status_callback)
        
        final_msg = f"{memory_result} / {knowledge_result}"
        print(f"--- [RAG] å‡¦ç†å®Œäº†: {final_msg} ---")
        return final_msg

    def search(self, query: str, k: int = 10, score_threshold: float = 0.75, enable_intent_aware: bool = True) -> List[Document]:
        """
        é™çš„ãƒ»å‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¸¡æ–¹ã‚’æ¤œç´¢ã—ã€è¤‡åˆã‚¹ã‚³ã‚¢ã§ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¦çµæœã‚’çµ±åˆã™ã‚‹ã€‚
        
        [Phase 1.5+] Intent-Aware Retrievalå¯¾å¿œ:
        - ã‚¯ã‚¨ãƒªæ„å›³ã‚’åˆ†é¡ã—ã€Intentåˆ¥ã«é‡ã¿ä»˜ã‘ã‚’å‹•çš„ã«èª¿æ•´
        - é«˜Arousalè¨˜æ†¶ã¯æ™‚é–“æ¸›è¡°ã‚’æŠ‘åˆ¶ï¼ˆæ„Ÿæƒ…çš„è¨˜æ†¶ã®ä¿è­·ï¼‰
        """
        results_with_scores = []
        
        # [Intent-Aware] ã‚¯ã‚¨ãƒªæ„å›³ã‚’åˆ†é¡
        if enable_intent_aware and self.api_key:
            intent_info = self.classify_query_intent(query)
            intent = intent_info["intent"]
            weights = intent_info["weights"]
            print(f"--- [RAG Search Debug] Query: '{query}' (Intent: {intent}, Threshold: {score_threshold}) ---")
        else:
            intent = constants.DEFAULT_INTENT
            weights = constants.INTENT_WEIGHTS[constants.DEFAULT_INTENT]
            print(f"--- [RAG Search Debug] Query: '{query}' (Intent: disabled, Threshold: {score_threshold}) ---")

        dynamic_db = self._safe_load_index(self.dynamic_index_path)
        if dynamic_db:
            try:
                dynamic_results = dynamic_db.similarity_search_with_score(query, k=k)
                results_with_scores.extend(dynamic_results)
            except Exception as e: print(f"  - [RAG Warning] Dynamic index search failed: {e}")

        static_db = self._safe_load_index(self.static_index_path)
        if static_db:
            try:
                static_results = static_db.similarity_search_with_score(query, k=k)
                results_with_scores.extend(static_results)
            except Exception as e: print(f"  - [RAG Warning] Static index search failed: {e}")

        # [Intent-Aware] 3é …å¼è¤‡åˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°:
        # Score = Î± Ã— similarity + Î² Ã— (1 - arousal) + Î³ Ã— (1 - decay) Ã— (1 - arousal)
        # - Î±: é¡ä¼¼åº¦ã®é‡ã¿
        # - Î²: Arousalã®é‡ã¿ï¼ˆé«˜Arousal = é‡è¦ãªè¨˜æ†¶ï¼‰
        # - Î³: æ™‚é–“æ¸›è¡°ã®é‡ã¿ï¼ˆé«˜Arousalã§æŠ‘åˆ¶ï¼‰
        alpha = weights["alpha"]
        beta = weights["beta"]
        gamma = weights["gamma"]
        
        scored_results = []
        for doc, similarity_score in results_with_scores:
            arousal = doc.metadata.get("arousal", 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5ï¼ˆä¸­ç«‹ï¼‰
            time_decay = self.calculate_time_decay(doc.metadata)  # 0.0~1.0ï¼ˆæ–°ã—ã„ã»ã©é«˜ã„ï¼‰
            
            # 3é …å¼è¤‡åˆã‚¹ã‚³ã‚¢:
            # - é¡ä¼¼åº¦ã¯ä½ã„ã»ã©è‰¯ã„ï¼ˆL2è·é›¢ï¼‰
            # - Arousalã¯é«˜ã„ã»ã©è‰¯ã„ â†’ (1 - arousal) ã§åè»¢
            # - æ™‚é–“æ¸›è¡°ã¯æ–°ã—ã„ã»ã©è‰¯ã„ â†’ (1 - decay) ã§å¤ã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£
            # - ãŸã ã—é«˜Arousalè¨˜æ†¶ã¯ (1 - arousal) ã§æ¸›è¡°ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è»½æ¸›
            time_penalty = (1.0 - time_decay) * (1.0 - arousal)  # Arousalé«˜ã„ã¨æ¸›è¡°ç„¡åŠ¹åŒ–
            composite_score = alpha * similarity_score + beta * (1.0 - arousal) + gamma * time_penalty
            
            scored_results.append((doc, similarity_score, arousal, time_decay, composite_score))
        
        # è¤‡åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        scored_results.sort(key=lambda x: x[4])
        
        # [2026-01-10 è¿½åŠ ] ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡é™¤å»
        seen_contents = set()
        unique_results = []
        duplicate_count = 0
        for doc, sim_score, arousal, decay, comp_score in scored_results:
            # å…ˆé ­100æ–‡å­—ã§é‡è¤‡åˆ¤å®šï¼ˆå®Œå…¨ä¸€è‡´ã§ã¯ãªããƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹æ¯”è¼ƒï¼‰
            content_key = doc.page_content[:100].strip()
            if content_key not in seen_contents:
                seen_contents.add(content_key)
                unique_results.append((doc, sim_score, arousal, decay, comp_score))
            else:
                duplicate_count += 1
        
        if duplicate_count > 0:
            print(f"  - [RAG] é‡è¤‡é™¤å»: {len(scored_results)}ä»¶ â†’ {len(unique_results)}ä»¶ ({duplicate_count}ä»¶é™¤å»)")

        filtered_docs = []
        arousal_boost_count = 0
        for doc, sim_score, arousal, decay, comp_score in unique_results:
            is_relevant = sim_score <= score_threshold
            clean_content = doc.page_content.replace('\n', ' ')[:50]
            status_icon = "âœ…" if is_relevant else "âŒ"
            
            # ArousalãŒé«˜ã„å ´åˆã¯â˜…ãƒãƒ¼ã‚¯ã€DecayãŒé«˜ã„å ´åˆã¯ğŸ†•ãƒãƒ¼ã‚¯
            markers = ""
            if arousal > 0.6:
                markers += " â˜…"
                arousal_boost_count += 1
            if decay > 0.9:
                markers += " ğŸ†•"
            
            print(f"  - {status_icon} Sim: {sim_score:.3f} | Arousal: {arousal:.2f} | Decay: {decay:.2f} | Comp: {comp_score:.3f}{markers} | {clean_content}...")
            
            if is_relevant:
                filtered_docs.append(doc)
        
        if arousal_boost_count > 0:
            print(f"  - [RAG] é«˜Arousalè¨˜æ†¶: {arousal_boost_count}ä»¶ãŒãƒ–ãƒ¼ã‚¹ãƒˆå¯¾è±¡")

        return filtered_docs[:k]